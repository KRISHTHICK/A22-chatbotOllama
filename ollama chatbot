# ===============================
# Required Packages (pip install)
# ===============================
# pip install streamlit
# pip install transformers
# pip install torch

import streamlit as st
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

@st.cache_resource
def load_model():
    model_name = "tiiuae/falcon-7b-instruct"  # Replace with a locally compatible HF model if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
    return pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

# Streamlit App
st.title("ðŸ¤– Hugging Face AI Chatbot (Local via Transformers)")
st.markdown("Runs fully locally using Hugging Face Transformers. No API involved.")

chat_pipeline = load_model()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.text_input("You:", placeholder="Ask a question...", key="user_input")

if user_input:
    full_prompt = "\n".join([f"User: {q}\nBot: {a}" for q, a in st.session_state.chat_history]) + f"\nUser: {user_input}\nBot:"
    response = chat_pipeline(full_prompt, max_new_tokens=200, do_sample=True, temperature=0.7)[0]["generated_text"]
    answer = response.split("Bot:")[-1].strip()
    st.session_state.chat_history.append((user_input, answer))

if st.session_state.chat_history:
    for q, a in reversed(st.session_state.chat_history):
        st.markdown(f"**You:** {q}")
        st.markdown(f"**Bot:** {a}")
